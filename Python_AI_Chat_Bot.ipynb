{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Chat Bot in Python"
      ],
      "metadata": {
        "id": "pwICHyLQg6ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary packages and load data into notebook"
      ],
      "metadata": {
        "id": "bdukUJ_mg2I2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub5Rt41Xyn-r"
      },
      "outputs": [],
      "source": [
        "# These packages currently only work in Python 3.6\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load json file into notebook\n",
        "with open('intents.json') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "JGqxTGvwyzwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data"
      ],
      "metadata": {
        "id": "Io2F5yWDgaLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting data from json file to put into lists.\n",
        "words = []  # unique words in patterns\n",
        "labels = [] # intent in json file\n",
        "docs_x = [] # tokenized words\n",
        "docs_y = [] # tokenized tage\n",
        "\n",
        "# looping through json and extracting data.\n",
        "# for each pattern, tokenize the words and add them to docs_x and their tags into docs_y.\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        wrds = nltk.word_tokenize(pattern)\n",
        "        words.extend(wrds)\n",
        "        docs_x.append(wrds)\n",
        "        docs_y.append(intent[\"tag\"])\n",
        "\n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])"
      ],
      "metadata": {
        "id": "RGlFLSNeyxf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "qBqtjvbOy6g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create unique list of stemmed words for data preprocessing\n",
        "#\n",
        "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# sort the labels list\n",
        "labels = sorted(labels)"
      ],
      "metadata": {
        "id": "ciwcjS9hwwou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reformat input for neural network\n",
        "# represent each sentence with list of length of amount of words in model's vocabulary\n",
        "training = []\n",
        "output = []\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "for x, doc in enumerate(docs_x):\n",
        "    bag = []\n",
        "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "    for w in words:\n",
        "        if w in wrds:\n",
        "            bag.append(1)\n",
        "        else:\n",
        "            bag.append(0)\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)"
      ],
      "metadata": {
        "id": "zEvtsFB7voj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert training data and output to numpy arrays\n",
        "training = numpy.array(training)\n",
        "output = numpy.array(output)"
      ],
      "metadata": {
        "id": "vc7FUccsyyXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building AI Model"
      ],
      "metadata": {
        "id": "VC4Dh1xI6DKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model architecture\n",
        "tensorflow.reset_default_graph()  # gets rid of any previous settings\n",
        "\n",
        "# standard feed-forward neural network\n",
        "# one input layer, two hidden layers, one output layer\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])  # defining input shape that the model should expect\n",
        "net = tflearn.fully_connected(net, 8)   # first hidden layer\n",
        "net = tflearn.fully_connected(net, 8)   # second hidden layer\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")  # get probabilities for each output\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)"
      ],
      "metadata": {
        "id": "P9CVJkJDsUWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and save model\n",
        "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "# model.save(\"model.tflearn\")"
      ],
      "metadata": {
        "id": "ZUAZsGwtsxMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}